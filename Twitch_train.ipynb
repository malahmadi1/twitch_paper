{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyM9xkD7L-NJ",
        "outputId": "9f23e09d-c215-4b54-d6fe-99c30973797e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.9/dist-packages (from imblearn) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.22.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install imblearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjVzgFXqkv27",
        "outputId": "325ba71f-8a6e-4b30-fed9-7496e313c930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCTPiT8Cm_2x",
        "outputId": "16444aa8-96a3-4d1f-f6aa-45e4306e4a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RangeIndex(start=0, stop=4000, step=1)\n",
            "Irrelevant to Streamer    2555\n",
            "Relevant to Streamer      1445\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Twitch_Colab/twitch_messages.csv')\n",
        "print(df.index)\n",
        "label_counts = df['label'].value_counts()\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nS7NZjGkMRYL",
        "outputId": "498eaca5-aa4d-451b-c1e4-44524d0e5ed6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-be4ea1becc2a>:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_spaces'] = df['replaced_author'].str.replace(r'\\s+', ' ')\n",
            "<ipython-input-5-be4ea1becc2a>:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_punctuation'] =  df['message'].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0                                                  ohhhhh\n",
            "1             If i set my legs on fire will i run faster?\n",
            "2                                              aaayyyeeee\n",
            "3       entobiBugLove entobiBugLove entobiBugLove ento...\n",
            "4          @user thank you for the gift sub!! gopiraHeart\n",
            "                              ...                        \n",
            "3993                                      just subscribed\n",
            "3994       you can also do go doc strings | grep contains\n",
            "3995                                                   Yo\n",
            "3997                               PORQUEEEEEEE MARIAAAA?\n",
            "3999            I get so stoked when I see you streaming.\n",
            "Name: replaced_author, Length: 3404, dtype: object\n",
            "before sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "author name was replaced with @author 36 times\n",
            "user name was replaced with @user 99 times\n",
            "After Sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'text': ['ohhhhh',\n",
              "  'If i set my legs on fire will i run faster',\n",
              "  'aaayyyeeee',\n",
              "  'entobiBugLove entobiBugLove entobiBugLove entobiBugLove entobiBugLove entobiNasiPog entobiBugLove entobiBugLove entobiBugLove entobiBugLove entobiBugLove entobiBugLove entobiBugLove',\n",
              "  'leor0111 thank you for the gift sub gopiraHeart',\n",
              "  'gopiraSip',\n",
              "  'Hello how are you today',\n",
              "  'Enderscram game dev is a small world thats why',\n",
              "  'anyone know of rose of starcross  am i interrupting something ',\n",
              "  'do you support lgbtq',\n",
              "  'gopiraDoggers',\n",
              "  'I love that it can be both a chemistry and guacamole pun',\n",
              "  'so Thor what about that secret beatem up you were working on last year is it finished is it published I lost a couple streams so I didnt got any news about it',\n",
              "  'NAME',\n",
              "  'choose  the bottom yes',\n",
              "  'I support everyone but Thor',\n",
              "  'nice',\n",
              "  'LUL',\n",
              "  'Thank you blumberquack',\n",
              "  'updates',\n",
              "  'I forgot how incredible that animation is LUL',\n",
              "  'hey GoPirateSoftware whats the update on the game dev category',\n",
              "  'oh hey i got a sub gifted thank you leor0111',\n",
              "  'ü•ö ',\n",
              "  'aww Ive only been here two minutes',\n",
              "  'entobiNerdBird entobiNerdBird entobiNerdBird entobiNerdBird entobiNerdBird entobiNerdBird',\n",
              "  'virus6Pog virus6Pog GMS2',\n",
              "  'it does xo220c',\n",
              "  'Maybe disable chroma key temporarily',\n",
              "  'beenatural You too',\n",
              "  'melynUh melynEz melynUh melynEz melynUh melynEz melynUh melynEz melynUh melynEz melynUh',\n",
              "  'hey guys how are you doing today',\n",
              "  'i mean were not stopping you KappaPride',\n",
              "  'oh congrats',\n",
              "  'A correctly meowing cat',\n",
              "  'leor0111 oh god thank you so much ',\n",
              "  'I played a bit a long time ago not 100 sure why I did not continue',\n",
              "  'This is goingto be a great game for the switch',\n",
              "  'eve',\n",
              "  'Can I make prank calls with dads phone',\n",
              "  'dark mode in notepad is way more better',\n",
              "  'Official Pirate Software Merch httpsgopiratesoftwarecomstore',\n",
              "  'reminds me igotta stream more NotLikeThis',\n",
              "  'I dont click Im scared if its rickroll',\n",
              "  'Oh hell yea PogChamp',\n",
              "  'maybe',\n",
              "  'cat cat cat',\n",
              "  'i found the face i shall use for your spiderbody',\n",
              "  'that cutscene made my stream lag LOL',\n",
              "  'i would die for terry',\n",
              "  'thats ',\n",
              "  'shiaUHH really',\n",
              "  'I tried it out but Im not in a good spot to pay for a subscription and I decided Id rather wait until I was',\n",
              "  'lovemefeedmeneverleaveme gifted a Tier 1 sub to hikariplays They have given 141 Gift Subs in the channel',\n",
              "  'thats by far the best game i ever saw made in gamemaker',\n",
              "  'Hell yeah Lemons',\n",
              "  'Hey Thor Good to catch you streaming again',\n",
              "  'Who arent we beating and why are we joining them',\n",
              "  'greshe',\n",
              "  'One of my snails is crawling on another snail',\n",
              "  'Skittles Mystery Edition',\n",
              "  'mystery skittles',\n",
              "  'woah i havent watched a stream in a while and that cutscene looked REALLY cool',\n",
              "  'httpsclipstwitchtvBetterSnappyDurianUWot1Bqcr9Ey26Qs1S',\n",
              "  'entobiGoodGirl entobiGoodGirl entobiGoodGirl entobiGoodGirl entobiGoodGirl entobiGoodGirl entobiGoodGirl entobiGoodGirl entobiGoodGirl',\n",
              "  'Doing great Its great to see you on 3 was talking about insect physiology on stream haha',\n",
              "  'cya bitrate',\n",
              "  'Hi thor',\n",
              "  'well met',\n",
              "  'LeonTheDapperShark subscribed at Tier 1 Theyve subscribed for 12 months Greetings friends o',\n",
              "  '3',\n",
              "  'LittleHam subscribed at Tier 1 Theyve subscribed for 33 months gopiraBrainChamp gopiraAxe Time to get rid of this',\n",
              "  'Want what Djinnet Kappa',\n",
              "  'grey grey grey grey grey and grey I wonder what grey tastes like tho',\n",
              "  'Settings Style Configuration Obsidian  Dark mode Notepad  which is much better',\n",
              "  'D',\n",
              "  'thor where are the ants',\n",
              "  'Even in the magical space void it‚Äôs still raining',\n",
              "  'who needs bitrate',\n",
              "  'Any more Trinidad Evil Pepper adventures since the last stream Thor',\n",
              "  'XD',\n",
              "  'leor0111 gifted a Tier 1 sub to buggles',\n",
              "  'heartbound mod that tazers you when you take damage Kappa',\n",
              "  'I want to steal that',\n",
              "  'yay',\n",
              "  'hey thor',\n",
              "  'yup i just did D',\n",
              "  'to lard and high in calories',\n",
              "  'leor0111 Thanks for the gift sub',\n",
              "  'Also 4 months what in the geck',\n",
              "  'you dont use darkmode on np',\n",
              "  'I heard you are the developer of heartbound its real',\n",
              "  'WE BROKE HIM',\n",
              "  '',\n",
              "  'Hmm 223 Does it meant its not the true limit bajoWow',\n",
              "  'admins quick reboot thor',\n",
              "  'Shiny',\n",
              "  '696 views nice',\n",
              "  'Do it ppixOwo',\n",
              "  'put the tower logo at 299',\n",
              "  'What do you call a fish wearing a bowtieSofishticated gopiraCheer50',\n",
              "  'i was eating dangit',\n",
              "  'Honestly being compared to other games at least for me is nice Cause Im like You think Its like OMORI Im flattered as all hell',\n",
              "  'holy moly',\n",
              "  'First time I ever said the f word I was never exposed todidn‚Äôt know what it was what it was so I pride myself on inventing it lmao',\n",
              "  'Thats amazing',\n",
              "  'absolute banger',\n",
              "  'Yeah ngl some of our first thoughts was white pride lmao dunno if it was a great design choice',\n",
              "  'you must take the fishing rod from underneath the dock to catch the red hitbox',\n",
              "  '19 raiders from entobird have joined',\n",
              "  'I never learned much about the ARG Is there anything at all you can say about it',\n",
              "  'Kerrrzo',\n",
              "  'what did I just witness',\n",
              "  'Kreygasm1 Kreygasm1 Kreygasm1 Kreygasm1 Kreygasm1',\n",
              "  'Hate when stuff like that happens',\n",
              "  'i like those trees with the hole',\n",
              "  'good morning everyone or afternoonevening depending on where you are',\n",
              "  'Much more organized than my notes',\n",
              "  'supportmodstaff subscribed with Prime Theyve subscribed for 2 months currently on a 1 month streak have my babies daddy i love you',\n",
              "  'If you keep dying Lore remembers how bad you are',\n",
              "  'haha',\n",
              "  'YARR Check out entobird over at httpswwwtwitchtventobird ',\n",
              "  'hon hon hon where did I park my baguette',\n",
              "  'sad',\n",
              "  'this man has no chill',\n",
              "  'Thor Have ya heard of rightdog Its the cousin of leftdog He left sorry for an terrible joke the8bitFine ',\n",
              "  'leor0111 gifted a Tier 1 sub to AlchyApples',\n",
              "  'hello',\n",
              "  'gopiraCheer100 hashtag Phat comeatmebro hashtag brian4life',\n",
              "  'You almost got it right the first time luna',\n",
              "  'Anyways hi lol',\n",
              "  'yes ant',\n",
              "  'awwww',\n",
              "  'Chelsea is better than Manchester city',\n",
              "  'My son recently discovered Heartbound while browsing games and really enjoyed it as did I I just wanted to stop in to tell you that',\n",
              "  'confusing but cool',\n",
              "  'damn I have to go bid on the game mechanic as an NFT',\n",
              "  'bomb',\n",
              "  'I just noticed my total gifted subs oops rooVV',\n",
              "  'that is indeed dumb',\n",
              "  'and were at 100 viewers',\n",
              "  'Or do nothing',\n",
              "  'Pretty dope',\n",
              "  'Kill The Moon is PirateSoftwares roguelike dungeon crawler that is developed live on stream httpswwwgopiratesoftwarecomgamesKillTheMoon',\n",
              "  'MYAAA MYAAA MYAAA MYAAA MYAAA MYAAA',\n",
              "  'and in space slapSmug people can shoot you',\n",
              "  'Hey ItsHoltzzy o',\n",
              "  'Ants',\n",
              "  'Eve is one of the games that sounds very interesting to me but not sure how much fun I would actually have Especially since I also want to play other SP games',\n",
              "  'wait you can delete rhode',\n",
              "  'that way youll also know the source',\n",
              "  'Thor sounds like a Triple A dev doing a youtube devlog',\n",
              "  'So mole friends name is Avocado because of Avogadros number right',\n",
              "  'ktm Dhalucario This will probably be next on the list',\n",
              "  'i love terry',\n",
              "  'skittle tastes like skittle I can‚Äôt tell beyond that',\n",
              "  'Give advice',\n",
              "  'pineapple pizza controversy incoming',\n",
              "  'Hm Maybe Ill come back Im just not sure where to start with it',\n",
              "  'I had horrible presyncope from whichever one I got its finally gone away now ',\n",
              "  'Oh god Eve',\n",
              "  'how to be A SUSSY BAKA OMGZGHEEKDVIDBFKEBEOFKD',\n",
              "  'Heck',\n",
              "  'okk',\n",
              "  'Feel like Im play the old hack games',\n",
              "  'holy gift bomb wall o chat',\n",
              "  'Shadeypup subscribed at Tier 1 Theyve subscribed for 41 months yo yo yo wassup buddy',\n",
              "  'dlnoboNice',\n",
              "  'why doesnt wow have fast travel',\n",
              "  'garlic is love 3',\n",
              "  'VampireRamen LOL',\n",
              "  'PogChamp',\n",
              "  'maxipaxitaxi subscribed with Prime Theyve subscribed for 6 months currently on a 1 month streak',\n",
              "  'Does Terry music pop up instantly or is it supposed to fade',\n",
              "  'Youre just blushing We get it Youre nervous around us',\n",
              "  'Hey just found your stream what is the game about',\n",
              "  'how far are you into the development',\n",
              "  'Its a cactJAM catJAM',\n",
              "  'and everyone',\n",
              "  'smh I hate it when my textures get too lard',\n",
              "  'Pog who is this wonderous strimmer',\n",
              "  'hikariplays I feel like I should know more about Heartbound to understand this All my exposure is like 3 streams',\n",
              "  'Literally just about to walk out the door for the parttime job but saw you were on and wanted to ask whether there was an update on Twitchother platforms',\n",
              "  'thats a lesson for politicians',\n",
              "  'Vote',\n",
              "  'Also Herro Thor and Thoths D',\n",
              "  'ohh üëÄ',\n",
              "  'same with like RTS',\n",
              "  'gopiraCheer65 glad to see you back but 1 pound of mac and cheese is a rookie number',\n",
              "  'why is thor talking with his mouth shut',\n",
              "  'all i can think of now is the mole scene from austin powers',\n",
              "  'toasts are deprecated use snackbars Kappa',\n",
              "  'motd',\n",
              "  'Didn‚Äôt you use Vim in the past',\n",
              "  'wil tofu bit anthbonk in the pee pee',\n",
              "  'swipe up',\n",
              "  'rewrite in Kotlin yes',\n",
              "  'can joose get 5 badges',\n",
              "  'joke',\n",
              "  'sup rockstar ‚úåüòÄ',\n",
              "  'what‚Äôs kenken',\n",
              "  'how come github so slow lately',\n",
              "  'i might download this',\n",
              "  'discord',\n",
              "  'alt enter ',\n",
              "  'Anthony I really cant find a solution to the problem im facing Is there a way to implement Vim key binding in a PySide multi line text widget If you know anything about this Please give me a tip',\n",
              "  'It‚Äôs probably Norton360',\n",
              "  'luplup112 subscribed with Prime Theyve subscribed for 7 months Well someone looks HOT',\n",
              "  'Hey chat 3',\n",
              "  'Bad bot',\n",
              "  'logcat should have a stack trace',\n",
              "  'D',\n",
              "  'if if if if if if if if if if if if if if',\n",
              "  'what the kotlin',\n",
              "  'what wrong with w3 schools',\n",
              "  'today',\n",
              "  'VoHiYo',\n",
              "  'What is kenken',\n",
              "  'Man its been a hot minute since I worked with Android PreKotlin',\n",
              "  'is this an old project ',\n",
              "  'This is super nice now',\n",
              "  'Its an Open Source App Store  for Open Source apps working without google',\n",
              "  'cool tnx',\n",
              "  'Sounds like a DSLR camera',\n",
              "  ' binbash',\n",
              "  'WOAH\\\\',\n",
              "  'did your plan work',\n",
              "  'are you working on windows today',\n",
              "  'its dot use',\n",
              "  'MetaHearth subscribed with Prime Theyve subscribed for 3 months',\n",
              "  'its pink',\n",
              "  'Nyooom wanderbotWave',\n",
              "  'were here yes',\n",
              "  'hi',\n",
              "  'lol',\n",
              "  'u win',\n",
              "  'P',\n",
              "  'I showed some coordinates  I hope thats not your address LUL',\n",
              "  'Kappa',\n",
              "  'Let us sink for a while',\n",
              "  'you when you write java ',\n",
              "  'ALT6 in android studio will open up the Logcat tab which will give you live log output even error stack traces so you dont need to attach the debugger just make sure to select your apps package name in the dropdown filter in logcat',\n",
              "  'very important plushie mopSmile',\n",
              "  'senpos thats ICS',\n",
              "  'awcHelloHello',\n",
              "  'chatrank',\n",
              "  'The scope becomes the thing that is built before',\n",
              "  'Xithrius subscribed with Prime Theyve subscribed for 4 months currently on a 2 month streak Not Python Impossible',\n",
              "  'LUL LUL',\n",
              "  'bonus stream  much appreciated ',\n",
              "  'grandpasipad is gifting 1 Tier 1 Subs to anthonywritescodes community Theyve gifted a total of 41 in the channel',\n",
              "  'wow is this famous kotlin programmer anthony',\n",
              "  'Hello Hello How are you doing today',\n",
              "  'what class is itfileDescriptor',\n",
              "  'LUL marshaSOCKS',\n",
              "  'statsJohnson',\n",
              "  'bonkedrank',\n",
              "  'This is not python what is this heresy',\n",
              "  'httpswwwtwitchtvanthonywritescodeclipSmellyAthleticBunnyShazBotstixDAj92IBtgT3jrIq',\n",
              "  'every great programmer has to be sued or you just failed at your LIFE DIE',\n",
              "  'Password leaked Kappa',\n",
              "  'linux',\n",
              "  'I think Marsha would just turn on cute emote only mode',\n",
              "  'Lemorz56 Id say just start somewhere Its much easier finding a good architecture when you have a better idea of what your domain is',\n",
              "  'in that specific list',\n",
              "  'is it\\xa0 that hard to settup emulator on linux',\n",
              "  'marshaSOCKS NOT MUCH HOW ABOUT YOU ',\n",
              "  'Oh boy file stuff on android took a turn in the latest versions',\n",
              "  'im learning japanese those symbols didnt look like any sort of japanese symbols they looked more like chinese but its possible that theyre just a line of kanji i dont recognize',\n",
              "  'httpsonlyfanscomanthonywritescode',\n",
              "  'top5bonked',\n",
              "  'bybye 3',\n",
              "  'anthonywritescode sup sup man Long time',\n",
              "  'what is this',\n",
              "  'Unix is user friendly Its just very particular about who its friends are',\n",
              "  'enterprise level code',\n",
              "  'hello marshaSOCKS ',\n",
              "  'hello',\n",
              "  'Java 8 has FilesreadAllLines',\n",
              "  'bonk FallenCatto',\n",
              "  '',\n",
              "  'Do I correctly notice some tan on your face Going out a lot lately',\n",
              "  'followage',\n",
              "  'hey Marsha',\n",
              "  'printstackTrace',\n",
              "  'coinflip',\n",
              "  'do you also what a new version of koblin the message at the bottom kind drives me nuts mopCringe',\n",
              "  'Thats the phone get got',\n",
              "  'ahh its not a proper anthony stream without marsha being bonked lol',\n",
              "  'its there',\n",
              "  'Hard drives also go up some new crypto',\n",
              "  'Wrong You didnt start the stream intending to rewrite But we all know deep down you want to do it',\n",
              "  'kevinsjoberg',\n",
              "  'bonk',\n",
              "  'object X is like making a variable X of an anonymous class',\n",
              "  'thinking about swtiching from vim to babi',\n",
              "  'EVERYWHERE',\n",
              "  'andrewlanex you can just do bonk',\n",
              "  'Im tempted to do a flutter rewrite',\n",
              "  'Also you might not have permissions',\n",
              "  'after deadsnakes deadphone',\n",
              "  'i wish i was that cool',\n",
              "  'Is it just me or does watching coding streams make you want to go open up your editor',\n",
              "  'Oh Wow lol',\n",
              "  'hi how do you program your keyboard on the center',\n",
              "  'Oh I was imagining you looking at pokeporn on your phone mopSmile mopSip',\n",
              "  'outdatedversion subscribed at Tier 1 Theyve subscribed for 11 months currently on a 7 month streak awcCarpet',\n",
              "  'It feels w i d e s c r i p t',\n",
              "  'qoah nice tan dude',\n",
              "  'yeah thats why i was wondering android studio is for boomers flutter is the new zoomer tool xD',\n",
              "  'yeah Im pretty sure theyre using precommitcom',\n",
              "  'top5bonkers',\n",
              "  '97',\n",
              "  'Tobytearray is the equivalent',\n",
              "  'heck',\n",
              "  'hi anthony what are you building',\n",
              "  'hynix isnt that a zelda enemy mopT',\n",
              "  'Im basically 9 now',\n",
              "  'You can call a file picker through an intent yes',\n",
              "  'Hellooo D',\n",
              "  'lol you know youre doing Java if you end up on Baeldung or mkyongs blogs',\n",
              "  'catJAM',\n",
              "  'those xml autocompletions feel so nice hmm',\n",
              "  'i see',\n",
              "  'It takes ages to search anything in google with my virtual machine',\n",
              "  'surprise android stream PogChamp',\n",
              "  'httpsclipstwitchtvKnottyOpenAlbatrossAMPTropPunchzoFm3z9FPOERzv3V',\n",
              "  'good to see u again',\n",
              "  'this looks similar to cayleys table',\n",
              "  'awcBonk marshaSOCKS',\n",
              "  'ChiaCoin uses proof of work with disk space',\n",
              "  'predict if juiice can have 5 badges',\n",
              "  'factories LUL',\n",
              "  'Kernel crash',\n",
              "  'gj niyrme',\n",
              "  'kevinsjoberg',\n",
              "  'yeah lmao',\n",
              "  'hold down file',\n",
              "  'its a webiste',\n",
              "  'i had rx580 in my cart ready to be bought and few months later its like a billion dollars clearly exaggerated',\n",
              "  'twitter',\n",
              "  'wow',\n",
              "  'no',\n",
              "  'We can hear you',\n",
              "  'updated',\n",
              "  'Hey man I have a few side projects I want to code but I‚Äôm struggling with planning architecture and designing the system do you know any good resources to learn that',\n",
              "  'cant you release it on FDroid ',\n",
              "  'nooooooooooooo',\n",
              "  'lol hardJohnson',\n",
              "  'i like vip',\n",
              "  'NET was named NET so that it wouldnt show up in a Unix directory listing',\n",
              "  'AppContex is THE god object',\n",
              "  'compilers arent paid enough to fix code',\n",
              "  'Hello hello',\n",
              "  'how your virtual machine runs so fast',\n",
              "  'How would you 2FA on aws  BibleThump',\n",
              "  'hey wait whyd you bonk me',\n",
              "  'I am scared',\n",
              "  'kenken',\n",
              "  'python programmer experiences generics for first time 1356 AD',\n",
              "  'public override fun  doesnt sound like a good time',\n",
              "  'rename to txt',\n",
              "  'Logdtag message',\n",
              "  'grandpasipad gifted a Tier 1 sub to theendlessriver',\n",
              "  'Hope all is going well',\n",
              "  'on the second day of tofu the streamer gave to chat two tofu chomps and one derpy corgi mleping',\n",
              "  'Kevin stop spending money you dont have',\n",
              "  'man i need some help with MediaPlayer',\n",
              "  'tuning in at the end rip üòÖ',\n",
              "  'Right Shapes is awesome',\n",
              "  'Freya as always making good stuff',\n",
              "  'Tyker7 subscribed at Tier 1 Theyve subscribed for 30 months',\n",
              "  'it was a bit there but its fine now',\n",
              "  'hi voyunHey',\n",
              "  'i agree ray',\n",
              "  'yes',\n",
              "  'goth radar',\n",
              "  'charos14 gifted a Tier 1 sub to gulgnack',\n",
              "  'is there a performance overhead for changing the size of the mesh the shapes are drawn on',\n",
              "  'I we draw different shapes in immediate mode can they all be batched together in a single draw call For example is it better to draw all the quads then all the circles in two loops or can we draw in a single loop both circles and quads',\n",
              "  'andyHi',\n",
              "  'Moar circles should be pentagons',\n",
              "  'Shapes are scary',\n",
              "  'do worldspace shapes get drawn before or after TAA resolve',\n",
              "  'Ah yeah makes sense Thanks',\n",
              "  'i should clip this and send it to the affinity designer devs bc i need this',\n",
              "  'do you care if we archivedownload the VOD when ur done',\n",
              "  'Question about colors Say I want several shapes close together fex multiple spheres each with a color with some degree of transparency The problem I ran into was that each overlapping sphere would add its own color to the area where the spheres are overlapping so the final color on the area where theres an overlap was darker than each individual sphere and with enough discs you could barely see through any of them',\n",
              "  'Greetings shaped mortals',\n",
              "  'love the hexagonale grid',\n",
              "  'Neat',\n",
              "  'Jabo posted it on the discord',\n",
              "  'izzythepenguin Thank you I must be doing something wrong then I just cant see the show at all',\n",
              "  'So cool Thanks for the stream and for all the examples',\n",
              "  'Sucks that they cant be put on canvases Im trying to make UI windows in the style of Persona 5 and theyd be perfect for that sugoiSadguts',\n",
              "  'zargy for me thats also the only reason I would ever need vector graphics would love to do UI ',\n",
              "  'i dont understand the math but thats amazing lol',\n",
              "  'the torus one is rad',\n",
              "  'Shapes is 50 off BTW What a great time to get it vlambeerFish',\n",
              "  'Missed a chunk but will be coming back to this on youtube  Thanks a lot',\n",
              "  'love your streams acegikmo',\n",
              "  'is the code open source for those who buy',\n",
              "  'is it billboarded',\n",
              "  'wow theres a lot of dragon balls near you LUL',\n",
              "  'Lol',\n",
              "  'a sweeping radar screen',\n",
              "  'MisterHex subscribed with Prime Theyve subscribed for 2 months VoHiYo',\n",
              "  'Could you show how to use HDR color on any shapes  Thanks  ',\n",
              "  'Are these streamings available later on VOD',\n",
              "  'httpswwwyoutubecomcacegikmo',\n",
              "  '\\\\o',\n",
              "  'hey just arrived here did i miss something',\n",
              "  'Can you apply texture',\n",
              "  'clouds',\n",
              "  'I got shapes when it first released its an amazing tool 50 off is a steal Do yourself the favor and get it if you dont have it yet',\n",
              "  'probably already asked before  is shapes usable in webgl for PC and possibly mobile',\n",
              "  'animation curves are really nice üòÉ',\n",
              "  'acegikHeartTrans',\n",
              "  'I think a shapes 101 to start with would be helpful',\n",
              "  '69 viewers funny number',\n",
              "  'uuuuh',\n",
              "  'Unrelated to shapes any particular asset store sale recommendations',\n",
              "  'There are pretty nice playlists with copyright free music ',\n",
              "  'can you use lit shaders with shapes',\n",
              "  'Gameplicit subscribed at Tier 1',\n",
              "  'i made gta using vectors',\n",
              "  'can you animate shapes',\n",
              "  'baiii FutureMan',\n",
              "  'submarines have radar fro when they breach',\n",
              "  'Noots still sound like a made up thing no matter how much i learn about it',\n",
              "  'could draw the dot before the rings to save a draw call',\n",
              "  'Can there be another DrawCommandQueued or something that caches to draw batched',\n",
              "  'id guess the triangle uses a triangle',\n",
              "  'oh sorry nevermind',\n",
              "  'This is an amazing stream ',\n",
              "  'NYOOoooooom o',\n",
              "  'And the dots would move if you move the children gameobjects',\n",
              "  'why was there a missing dash',\n",
              "  'colasXD',\n",
              "  'I dont think that will ever hit 1',\n",
              "  'Would this affect other immediate drawing scripts',\n",
              "  'the dork in me is like aaaa thats not how radars do',\n",
              "  'Oop caught the tail end then',\n",
              "  'will you be uploading this vod to youtube ',\n",
              "  'HahaCat',\n",
              "  'Ill be third then p',\n",
              "  'Does it work in VR',\n",
              "  'beautiful',\n",
              "  'interesting',\n",
              "  'dont end the stream make something else please D',\n",
              "  'coxMuppet its okay learning to stream take a while',\n",
              "  'because shapes is cool',\n",
              "  'make it a teal ripple in wter',\n",
              "  'are u gonna upload this session in ur youtube channel',\n",
              "  'intersections ilnes in polygon',\n",
              "  'what do you mean this is a 2k viewers stream',\n",
              "  'Thank you that makes a lot of sense',\n",
              "  'Is there a component to draw a polyline between an array of Transforms so that then you could animate the Transforms not the array values',\n",
              "  'Any SVG support in the future for Shapes',\n",
              "  'This may be too off the rails for this tutorial but can you walk through some tips for using the frame debugger for analyzing performance of shapes',\n",
              "  'very cool',\n",
              "  'like a radial increase',\n",
              "  'HeyGuys',\n",
              "  'Is this asset going off of sale tonight with the other 2D items',\n",
              "  'If it breaks just leave a SOLID review on the asset store xoxo',\n",
              "  'Overview and different use cases',\n",
              "  'Speaking of cursors my mouse broke so now the cursor wont move',\n",
              "  'msuic has copyright issues',\n",
              "  'Shapes seems pretty useful',\n",
              "  'C',\n",
              "  'Maybe start with wherehow you should place and construct the script to get a line to show up on screen',\n",
              "  'Can these be used as particles',\n",
              "  'ok thanks',\n",
              "  'acegikmo  How are you  Any dreams of having Shapes render 3D objects  like saddles ',\n",
              "  'do you have any benchmark scenes set up for shapes',\n",
              "  'Super interesting',\n",
              "  'brb pc format',\n",
              "  'perfect',\n",
              "  'tapir2342 httpsgithubcomFreyaHolmerMathfs',\n",
              "  'I have learnt a lot from your streams',\n",
              "  'fancy hexagonal grid',\n",
              "  'You talking of the radius as smallish reminds me of how LaTeX works with qualitative font size',\n",
              "  'it did for a short peroid',\n",
              "  'noots rule',\n",
              "  'struct of arrays vs array of structs',\n",
              "  'it wouldnt be a good coding livestream without some trouble',\n",
              "  'from a completely performance perspective how would this compare to making a special shader just for radars that would just draw on a quad',\n",
              "  'Computers in 2021 tsodinCry',\n",
              "  'cool',\n",
              "  'when plugin the mouse',\n",
              "  'Your mic is turned off and on',\n",
              "  'This was very informative thank you ',\n",
              "  'hiall',\n",
              "  'henlo',\n",
              "  'Hi everyone gamepl16Aligatte',\n",
              "  'thanks for this asset i like it a lot',\n",
              "  'charos14 is gifting 3 Tier 1 Subs to Acegikmos community Theyve gifted a total of 3 in the channel',\n",
              "  'I couldnt find an answer on the docs so will ask here  when you do a dashed line in a 3D that goes into the distance will the dashes get smaller accordingly or is it all screenspace sized',\n",
              "  'ping',\n",
              "  'Move the gameobjects outside the ring ',\n",
              "  'Id love an overview to start I tried using it recently and got a little lost',\n",
              "  'I see thanks',\n",
              "  'what is local space',\n",
              "  'hello  im here because i love programming im glad that you are doing that ',\n",
              "  'Hey Freya I dont even use Unity Nonetheless your content is amazing and very interesting Congrats for the great job',\n",
              "  'wait whats happening with those two circles are they going through each other it doesnt look like it in terms of occlusion',\n",
              "  'can you put shapes in the unity canvas',\n",
              "  'is Shader Forge free',\n",
              "  'thanks',\n",
              "  'Are shapes actually just meshes as in could you export them as say an OBJ',\n",
              "  'i‚Äôll claim 4th place',\n",
              "  'lucasgoesme subscribed with Prime',\n",
              "  'Thats a great idea ',\n",
              "  'hello everyone',\n",
              "  'hey im an intermediate mobile developer with like 1 year experience and im interested in game dev is it worth shifting career this early or not really',\n",
              "  'now we can draw our Ping Pentagon ',\n",
              "  'drops notes binder in the stairs between rows of the lecture hall',\n",
              "  'Is this going to be available as a VOD',\n",
              "  'Hello',\n",
              "  'chandlerm35 definitely works in 20211 I have it in 20211 and it works just fine',\n",
              "  'howlin32Sharkpog',\n",
              "  'suwWhaa',\n",
              "  'Is there no sound yet',\n",
              "  'just unplug the cable D',\n",
              "  'Whats the performance difference between GO shapes and Immediate mode',\n",
              "  'haha yeah',\n",
              "  'you are a 200 viewer talent',\n",
              "  'thatll be cool',\n",
              "  'is it rendered both sides of the object',\n",
              "  'until when is shapes on sale',\n",
              "  'cursed 100',\n",
              "  'ah yes everyone knows radar is so important on submarines so common',\n",
              "  'yea',\n",
              "  'how tricky would it be to generate a collision box from a shape',\n",
              "  'bezier curves',\n",
              "  'An overview would be great',\n",
              "  'yeah yeah',\n",
              "  'much more naturalistic',\n",
              "  'Applause',\n",
              "  'long time no see hey HeyGuys how have you been',\n",
              "  'Hi Small question does shapes only use hardware AA or you coded distance fields for parts of the line segment join cap ',\n",
              "  'Combining vectors are my friend',\n",
              "  'ooooh so its like scaling the line as a whole',\n",
              "  'is there a small gap between the monitors in windows I had an issue like that on my work machine a few weeks ago',\n",
              "  'When using the gradient fill do all the vertices have a color or the gradient is computed in the pixel shader',\n",
              "  'Aha I usually watch on youtube but now Im here Through it looks like I missed the beginning',\n",
              "  'hello yall 3 3 3 3',\n",
              "  'KonCha hellouw KonCha',\n",
              "  'Are those for detecting kittens acegikMeow',\n",
              "  'haha my cs professor drew with her mouse during zoom classes',\n",
              "  'ohh so you cant use immediate mode without inheriting from that class',\n",
              "  'thiccc boi',\n",
              "  'Sorrry i made this go off topic',\n",
              "  'Tomorrow war was ok Had some things they didn‚Äôt explain very well or at all',\n",
              "  'MarvelScorpion we havent used it yet',\n",
              "  'oh yeah sexlife is one weird show',\n",
              "  'thanks',\n",
              "  'evernote ',\n",
              "  'beastco gifted a Tier 1 sub to BetaFolf',\n",
              "  'expect a hot tub stream any minute now',\n",
              "  'vinnycode itsjay20LOVE waddap homie',\n",
              "  'what are your thoughts on github copilot',\n",
              "  'really badly',\n",
              "  'and that yoiu are not good enough',\n",
              "  'familyfriendly stream of DICKS',\n",
              "  'Yeahhhh but code snippet in Craft is annoying',\n",
              "  'okay thanks',\n",
              "  'ok bye D',\n",
              "  'drop',\n",
              "  'bjj',\n",
              "  'how moving to yarn fixed vs code auto imports Mine are broken',\n",
              "  'is there something with head Kappa',\n",
              "  'Did I miss something',\n",
              "  'email',\n",
              "  'I joined at a weird moment',\n",
              "  'monkaS',\n",
              "  'Im tailwind Nice',\n",
              "  'Will you ever not use Tailwind',\n",
              "  'wuzdat',\n",
              "  'Kreygasm',\n",
              "  'httpswwwremnoteio',\n",
              "  'You have to manually do it with Notion and it gets frustrating',\n",
              "  'nah closer',\n",
              "  'Notion for the win',\n",
              "  'add a walking duck MingLee',\n",
              "  'can anybody tell why we reset constructor in javascript',\n",
              "  'I wonder what twitch ToS have to say about this',\n",
              "  'byeeeee',\n",
              "  'Yep Jordan ASS is so big',\n",
              "  'netflix',\n",
              "  'tomorrow',\n",
              "  'what is your fav stack',\n",
              "  'undo',\n",
              "  'so when your d grows your butt shrinks',\n",
              "  'wtf tru',\n",
              "  'mechanically Kappa',\n",
              "  'animation looks nice',\n",
              "  'is the core functionality of the product already together',\n",
              "  'I dont mean to distract but I thought this was a mellow remix of Jamaican Hits',\n",
              "  'kevinsjoberg Yeah thought so Ive never tried Roam myself but Ive heard good things',\n",
              "  'theme',\n",
              "  'biig Kappa',\n",
              "  'i would love this same energy and overthinking but on something random like your favorite pen',\n",
              "  'If there was one stream for me to catch this one was a good choice',\n",
              "  'will you be able to search for everything or only folderssubjects',\n",
              "  'i am so glad that i didnt miss this LUL',\n",
              "  'who hates jira',\n",
              "  'you are using custom nextjs server right',\n",
              "  'the imports are kinda broken with new vscode update tho',\n",
              "  'apple users xD pgup pgdown',\n",
              "  'sleiphyr no its not anymore',\n",
              "  'Wait when would you not use tailwinf',\n",
              "  'What is that mock email tool',\n",
              "  'age',\n",
              "  'Looks good ship it',\n",
              "  'suck',\n",
              "  'What note app is he using',\n",
              "  'Does Roam allow for cancelling a todo like PlainTasks does It seems so rare in notetaking app that does todos',\n",
              "  'wheres the simp king phvn at',\n",
              "  'isnt a map faster',\n",
              "  'a barrel roll animation',\n",
              "  'I want to create a test framework and call it Kim Passable',\n",
              "  'RealEmail is an email service that were building as a competitor to heycom It allows users to get email addresses ending with realemail Yes this is a real thing I am building See stack for details about the tech stack',\n",
              "  'today start is a new high Kappa',\n",
              "  'That‚Äôs really practical for like tight pants and shit though',\n",
              "  'headin out good luck with stream today',\n",
              "  'emailme',\n",
              "  'beastco gifted a Tier 1 sub to Slackercat777',\n",
              "  'I hope so I need to listen to it again take care yall',\n",
              "  'notion is good',\n",
              "  'Gonna go on a walk Cya ',\n",
              "  'Oof',\n",
              "  'c17r what do you mean by cancelling',\n",
              "  'The tech stack is all scaffolded with Nytro httpsnytrodev Core technologies TypeScript React NextJS Prisma postgres GiraphQL Relay',\n",
              "  'Talking about dicks just earned him money',\n",
              "  'oho thats interesting',\n",
              "  'pp',\n",
              "  '',\n",
              "  'theme',\n",
              "  'my peen is avg but my default peen is bigger than yours',\n",
              "  'i hope it works well with tabnine when it releases',\n",
              "  'i dont have any sound',\n",
              "  'friday',\n",
              "  'Very cool',\n",
              "  'function inheritC P  var F  function Fprototype  Pprototype Cprototype  new F Cuber  Pprototype Cprototypeconstructor  C  WHY  ',\n",
              "  'l LUL LUL LUL',\n",
              "  'is this the day jordan took shower and changed shirt FeelsGoodman',\n",
              "  'imposter syndrome  when you think you are worthless shit and you are not worthy the job you have',\n",
              "  'The large balls make the small pp look even smaller',\n",
              "  'VJJ is for content',\n",
              "  'if Index is 0 and you press 2x ArrowDown it will be 2',\n",
              "  'trashdev we went over this GROWER NOT A SHOWER',\n",
              "  'I wanted to watch code not listen to Dr Cockter overhere',\n",
              "  'Same',\n",
              "  'Ive been using trello for everything  Am I weird',\n",
              "  'centos1235 Yeah Im already using Zettelketten for my note taking Its life changing',\n",
              "  'solved mewtru',\n",
              "  'I need tru to confirm this for science reasons',\n",
              "  'smol ONLY when flacid Kappa',\n",
              "  'do you use alfred for mac',\n",
              "  '',\n",
              "  'my default peen  jordans default peen when 20deg',\n",
              "  'stack',\n",
              "  'ü§£ü§£ü§£',\n",
              "  'the last 15 of the erecting is where the magic happens',\n",
              "  'whats extension for importing stuff',\n",
              "  'How many coworkers frequent here',\n",
              "  'nytro',\n",
              "  'this is kind of like how I use my moleskin but digitally thanks for the heads up on this',\n",
              "  'Is it like a concertina',\n",
              "  'Hello',\n",
              "  'Notion',\n",
              "  'How old are you ',\n",
              "  'that looks nice tho',\n",
              "  'const peenSize  001',\n",
              "  '1',\n",
              "  'no prn collection showcase today ',\n",
              "  'how are u programming on Trashdevs website',\n",
              "  'what about hscreen instead of minhscreen',\n",
              "  'Hai Tru',\n",
              "  'Hey man what routematerial did you use to become a full stack dev Complete noob here is the Odin project reputable',\n",
              "  'average notepad enjoyer here',\n",
              "  'course',\n",
              "  'how did this come up who asked what',\n",
              "  'beastco Thanks for the gift sub',\n",
              "  'Sorry still thinking about micropenises and stuff',\n",
              "  'drop itsjay20LOVE',\n",
              "  'c17r Its crossed off the list but as basically not gunna happen instead of done',\n",
              "  'its everywhere Brown',\n",
              "  'bro im just tryna watch a guy write code',\n",
              "  'peepoBlanket',\n",
              "  'So you guys have setup custom mail exchange server',\n",
              "  'Oh sheet trashdev is here mewtru We have to stop talking about him',\n",
              "  'vape is here to show off his full stack üòÇ',\n",
              "  'whats imposter syndrome',\n",
              "  'windy vanilla sprinkles',\n",
              "  'If there‚Äôs one thing we need more of it‚Äôs note taking apps',\n",
              "  'KEKW',\n",
              "  'next project build a note taking app',\n",
              "  'Oh',\n",
              "  'I like that speed',\n",
              "  'the amount of thought you put into things like this i respect it',\n",
              "  'better',\n",
              "  'o',\n",
              "  'VapeJuiceJordan more examples httpsuiwtfcmdk',\n",
              "  'Reviewing ass',\n",
              "  'remnote logseq athens  so many apps',\n",
              "  'I had a job interview this morning and now I have an online technical test to do üôè',\n",
              "  'twittercomVapeJuiceJordan',\n",
              "  'yes glass',\n",
              "  'thats a shit selling point',\n",
              "  'DA FCK BRO',\n",
              "  'drop laks1Pog',\n",
              "  'what are you doing',\n",
              "  'awcHelloHello awcCarpet',\n",
              "  'drop mewtruParty',\n",
              "  'primeagenEmacs',\n",
              "  'hahah kungfury theme',\n",
              "  'Just got notified on discord as well ',\n",
              "  'Sadge',\n",
              "  'whats the project',\n",
              "  'clear explanation of why he doesnt like angular right there',\n",
              "  'Sumdocsever',\n",
              "  'drop vjjWow',\n",
              "  'is giraphql a better nexus',\n",
              "  'Just write TODO in your code and it will work trust me Kappa',\n",
              "  'next',\n",
              "  'What does the pineapple thing mean for channel points',\n",
              "  'you forgot to post ur discord annoucement today i thought you lost ur streazk',\n",
              "  'Yesssirski',\n",
              "  'do you use glasses',\n",
              "  'TS is annoying',\n",
              "  'thats kinda lame dude',\n",
              "  'GivePLZ vjjDumptruck TakeNRG',\n",
              "  'thanks for walking through this Jordan',\n",
              "  'do people refer to you as VapeJuiceJordan internally at Netflix',\n",
              "  'Do you rewrite project again',\n",
              "  'pussslayer',\n",
              "  'Whole Noods was an OnlyFans competitor that we developed onstream The project is mostly functional but was missing payments Payment processing for high risk sites is challenging and we never figured out a good way to do payouts Because of this the project is currently on hold',\n",
              "  'my brain is scrambling to follow',\n",
              "  'Jordan is showing us how to make sure no one else can read your work',\n",
              "  'primeagenBig Good afternoon vapejuicejordan primeagenHands',\n",
              "  'mewtruFancy',\n",
              "  'it breaks',\n",
              "  'drop twitchRaid',\n",
              "  'or fastrefresh rather',\n",
              "  'imu',\n",
              "  'ending',\n",
              "  'Also hi D',\n",
              "  'hey',\n",
              "  'netlify vercel heroku',\n",
              "  'bless',\n",
              "  'JORDAN IS LIVE FOR AN HOUR',\n",
              "  'Any suggestions to try some crazy programming projects just for fun ',\n",
              "  'drop widepeepoHappy',\n",
              "  'Do you like redux',\n",
              "  'drop mewtruPls',\n",
              "  'I remember that being really annoying',\n",
              "  'yes all netflix data gets logged on his computer',\n",
              "  'h is for go back to canada melkey',\n",
              "  'mewtru hello',\n",
              "  'why am i seeing a route component i thought this was nextjs',\n",
              "  'Hi i just entered this stream looking for an answer in Web dev  is worth it to get a sub in google one and host my website in google cloud ',\n",
              "  'Selectthegang',\n",
              "  'HypeHeyFriends',\n",
              "  '6 DAYS LEFT 6 DAYS LEFT NotLikeThis',\n",
              "  'drop FutureMan',\n",
              "  'drop BibleThump',\n",
              "  'oof rounded cam looks sick',\n",
              "  'i like cheese',\n",
              "  'drop xqcS',\n",
              "  'drop 2020Drop',\n",
              "  'mindblown',\n",
              "  'in colegue ',\n",
              "  'wait  i run a gql api server in container aswell  but its not this slow',\n",
              "  'drop retrom23Heart',\n",
              "  'drop waffleBASS',\n",
              "  'What no tsignore does to a mf',\n",
              "  'i could even compile that mdx tlak',\n",
              "  'and then back end ',\n",
              "  'i guess linux native docker',\n",
              "  'danny3Hi',\n",
              "  'drop 2020Unroll',\n",
              "  'you can get that immediate dopamine',\n",
              "  'melkey pays his rent in tiktok exposure',\n",
              "  'your ben awad dm has 0 views tho',\n",
              "  'everyboy here for the drop LUL',\n",
              "  'I wouldve slapped as any onto the first red squiggly line I saw KEKW',\n",
              "  'dont remember the lighting being this good',\n",
              "  'Whats up yall',\n",
              "  'drop laiCalculated',\n",
              "  'VBISpencer subscribed with Prime Theyve subscribed for 4 months',\n",
              "  'Jordan acting like he in black ops 2 spawn killing everybody',\n",
              "  'no ubuntu',\n",
              "  'make a bot to detect ur live status',\n",
              "  'same but now it all makes sense BIGBRAIN',\n",
              "  'what makes real email better then the rest',\n",
              "  'drop Kappa',\n",
              "  'i guess cause its running in a vm',\n",
              "  'How did the chef‚Äôs food taste',\n",
              "  'would this app be better in angular',\n",
              "  'bruce wayne D',\n",
              "  'can i preorder the course so you can actually release it',\n",
              "  'EZ',\n",
              "  'That was a bit too energetic',\n",
              "  'VapeJuiceJordan do you think it is worth to test your code  by using like jest and enzyme',\n",
              "  'bet',\n",
              "  'How many times a week',\n",
              "  'hello mr and ms broadcaster',\n",
              "  'worst drop ever',\n",
              "  'you dont',\n",
              "  'drop PogChamp',\n",
              "  'hows it going',\n",
              "  'I wonder if something like concurrently \\\\tsc w\\\\ \\\\nodemon libindexjs\\\\ would be faster',\n",
              "  'but what about it lol I have used it for a long time which is why i dont get it',\n",
              "  'but hot reload in general is really fast',\n",
              "  'drop HahaSnowhal',\n",
              "  'passing what type of element as a prop like asdiv and such',\n",
              "  'VapeJuiceJordan what happened to noods man  i remember you spent time on it ',\n",
              "  'O',\n",
              "  'sometime i would love to here your technical breakdown of why you dont like angular always been curious to learn',\n",
              "  'drop 100',\n",
              "  'alperngd you can do it',\n",
              "  'Oh drop just doesnt work at all with bits',\n",
              "  'Wait md Im not that familiar with that stuff but dont you also write readmemd  What does it stand for',\n",
              "  'vercel is not afaik',\n",
              "  'thanks a lot guys i really appreciate it',\n",
              "  'drop waffleAYAYA',\n",
              "  'drop milkydPotato',\n",
              "  'Join the Discord httpsdiscordgg8Hya56K',\n",
              "  'not deep',\n",
              "  'hype',\n",
              "  'Today Im continuing to build my heycom competitor realemail For more details on the project use email',\n",
              "  'sounds really cool',\n",
              "  'Hi mewtru trashdev',\n",
              "  'oh ok got it',\n",
              "  'what‚Äôs jotai',\n",
              "  'notfirst',\n",
              "  'I do password guy',\n",
              "  'you are a tiktok start tho',\n",
              "  'ooop',\n",
              "  'yup Navlink is better for that',\n",
              "  'drop primeagenEmacs',\n",
              "  'What are you using to make those slides',\n",
              "  'yeah thats facts',\n",
              "  'so thats 1800 bill and a free candy before',\n",
              "  'I was about to point that out lol',\n",
              "  'Ive met him Hes friends with Jackalope Nice guy',\n",
              "  'httpsyoutubexrrj9Wc2L84 bring in the nerds right at the end',\n",
              "  'hello hello',\n",
              "  'Hey guys gopiraHey',\n",
              "  'mistshWave',\n",
              "  'Would have been less of a problem if the conversion tool hadcould have accounted for that',\n",
              "  'omg mrmr PridePog your so heking awesome',\n",
              "  'hello all hello ferrets',\n",
              "  '1010 Algorithm',\n",
              "  'if you like REALLY know nothing should we trust what anyone says at defcon Kappa',\n",
              "  'I just remembered Cow and Chicken That was a fun show',\n",
              "  'morning Thor KonCha',\n",
              "  'RisingDawnMc subscribed with Prime Theyve subscribed for 12 months',\n",
              "  'My favourite edition',\n",
              "  'Minesweeper zaqOMG',\n",
              "  'btw what made you decide to use game maker for heartbound in the first place',\n",
              "  'I did not know they were carnivores that will give me scary thoughts of ferrets neat',\n",
              "  'I made an app with Science  Technology streams but with most livecam type of stuff filtered Its not a separate category but its way easier to discover gamedev  programming streams A link is in uservoice comments search my nickname',\n",
              "  'This is a bad explanation',\n",
              "  'but what does it mean ',\n",
              "  'gopiraBrainChamp',\n",
              "  'good to know this games still going strongüëç',\n",
              "  'probably because lots of small files',\n",
              "  'Olives are pretty expensive',\n",
              "  'I like that solution',\n",
              "  'Hello Thor hello chat',\n",
              "  'gamemaker pog',\n",
              "  'which language is this',\n",
              "  'change of plans Thor  go do hot tub streams',\n",
              "  'howdy Thor',\n",
              "  'Speechless',\n",
              "  'whats a clown show',\n",
              "  'GoPirateSoftware At this point it feels like they arent making it ON PURPOSE',\n",
              "  'going from my 1st job to my second was the same thing ¬£10000 pay increase and less pressure I went from a 50 hour weekunpaid overtime to 375',\n",
              "  'The dog dies at the end',\n",
              "  'Hello good sir',\n",
              "  'dont you guys have phones apparently not',\n",
              "  'Recounting MMO stories ahhh very nice',\n",
              "  'its a space show that sold me',\n",
              "  'make a game called Game Development ',\n",
              "  'Corporate speak',\n",
              "  'sure',\n",
              "  'wow such wow',\n",
              "  'tragedy',\n",
              "  'hello am go pirate you game',\n",
              "  'All we ever ask',\n",
              "  'miyins and miyins of dialogue files',\n",
              "  'early access but yes',\n",
              "  'have a good stream clonzeLuv',\n",
              "  'The ferret fever has claimed your heart',\n",
              "  'Excuse me I need to see this',\n",
              "  'Its hard fighting with my friends in the military with how anti vax a lot of people are there',\n",
              "  'hey Thor would you recommend GMS2 over Godot for 2D',\n",
              "  'Cancel culture would have a blast with Old Cartoons',\n",
              "  'No that actually seems fair enough',\n",
              "  'Is your AP going psycho',\n",
              "  'thats a whole ferret sanctuary',\n",
              "  'drops a shovel in the chat so you can burry it',\n",
              "  'Starlight brigade',\n",
              "  'They are a kin to honey badgers remember this always',\n",
              "  'Some of the best engineers i know have trouble with it',\n",
              "  'Caffine1138 subscribed at Tier 1 Theyve subscribed for 36 months by all the gods 36 months Its good to see you Thor I hope everyone is doing well',\n",
              "  'Courage the Cowardly Dog',\n",
              "  'i miss justin tv LUL',\n",
              "  'LOL',\n",
              "  'but seems like it doesnt work on putting the end of the url',\n",
              "  'How long have you been developing this',\n",
              "  'which tools did you use as a tester GoPirateSoftware I start next week as an testautomatization dude and i still have no clue lol',\n",
              "  'wander brain bad at words',\n",
              "  'Ren and Stimpy was the shit',\n",
              "  'Seems like that help request didnt read ya message properly There were nothing to be confusion about bajoWtf',\n",
              "  'Wow Youre streaming And im not at work limesPoggers',\n",
              "  'Thats you Thor',\n",
              "  'Where people from different towns realize theyre all part of some bigger darker thing',\n",
              "  'üßÄ',\n",
              "  'Woodisch subscribed at Tier 1 Theyve subscribed for 44 months currently on a 44 month streak Ahoy matey',\n",
              "  'help pls',\n",
              "  'damn thast spooky',\n",
              "  'I wanted to ask this for a long time but what was the reason behind Pirate Software Did you not anticipate the Discord mess we have when you named it',\n",
              "  'Space Dandy is an Anime',\n",
              "  'Id hate to come home from work to that lol',\n",
              "  'I just wanted to see what Gir was always upto',\n",
              "  'The real question is if theyll be a translator for Klingon hmmm',\n",
              "  'The problem with learning is that it‚Äôs harder to do than just eating cake',\n",
              "  'so its out',\n",
              "  'Id you ever want to start using unity let me know there is some horrifying stuff that you only tend to figure out way too late',\n",
              "  'hey holtzy',\n",
              "  'i dont have insert',\n",
              "  'I wouldnt say thats true',\n",
              "  'morning guys how are you doing today',\n",
              "  'i wasnt here ',\n",
              "  'I cant hear the stream because I didnt grab my headphones but I am happy because I saw ferret',\n",
              "  'VK',\n",
              "  'shotpoop LUL',\n",
              "  'numbers youve never even seen',\n",
              "  'gopiraFear',\n",
              "  'im the one who cant spells good PridePog',\n",
              "  'Grad school kicking my butt',\n",
              "  'well at least they replied its something I guess',\n",
              "  'heya',\n",
              "  'you have a butterfly there maybe',\n",
              "  'Again Thats unheard of',\n",
              "  'I will buy it later',\n",
              "  'Thor Ive gone over 696k points Ive failed my mission',\n",
              "  'Join the official Discord discordggpiratesoftware',\n",
              "  'stirs bucket and shields eyes from the glass',\n",
              "  'your music rocks',\n",
              "  'Just added the discord',\n",
              "  'Pets are really free entertainment sometimes',\n",
              "  'Yeah its weird Twitch is not fixing it',\n",
              "  'Honestly I barely remember what I used to watch aside from Ed Edd n Eddy',\n",
              "  'both Philipp3',\n",
              "  'DELETE THE FRAMES',\n",
              "  'oh yeah Thor Have ya play the new patch in wow We can ride in the maw now bajoAssend',\n",
              "  'hi back my name is Philipp P MycahCambey',\n",
              "  'Uptime is a measure of system reliability expressed as the percentage of time a machine typically a computer has been working and available Uptime is the opposite of downtime',\n",
              "  'I dont think so',\n",
              "  'Heartbound',\n",
              "  'Hello Thor good morning from the east coast',\n",
              "  'thats about all I know about it',\n",
              "  'the time is still young',\n",
              "  'the internet brings up a valid point',\n",
              "  'so no cartoon easter eggs',\n",
              "  'How',\n",
              "  'sylessHype',\n",
              "  'Wheres your biggest owners from',\n",
              "  'gopiraHeart TELL ME ABOUT YOUR ART gopiraHeart',\n",
              "  'yep me too for the past 2 weeks',\n",
              "  'Or art as well',\n",
              "  'Heartbound is just made to brainwash the world',\n",
              "  'key words popular belief because a gold fish remember for longer than peoples think they do',\n",
              "  'sylessLaugh',\n",
              "  'Just screwing up my sleep schedule as usual on the weekend heh',\n",
              "  'just goin to bed',\n",
              "  'Hehey What we cooking today',\n",
              "  'breaks through the roof window and propels down into chat',\n",
              "  'a bit late though',\n",
              "  'I mean my comment was from what BigAsAHouse wrote I havent watched it',\n",
              "  'i dont need credit',\n",
              "  'I now graduated highschool and got into college',\n",
              "  'in a video game',\n",
              "  'nvm found it',\n",
              "  'Ayo Thor I havent had a chance to catch your streams lately woah',\n",
              "  'I dislike these massive corporations so much',\n",
              "  'wanna get a slower cooker gotta upgrade to the Bluetooth wireless rtx powered cooker',\n",
              "  'uuuh woow thats a great answer Thank you so much Mr Streamer ',\n",
              "  'What is your opinion on cryptocurrency',\n",
              "  'The amish are on their own',\n",
              "  'WHo wrote the game story',\n",
              "  '5 raiders from theartsypoe have joined',\n",
              "  'It looked pretty But its very early and crusty',\n",
              "  'Same argument for DOOM lol',\n",
              "  'Clip it print it out frame it lick it',\n",
              "  ...],\n",
              " 'labels': [1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  ...]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "#model_checkpoint = \"albert-base-v2\"\n",
        "\n",
        "# Which models can we use?\n",
        "# 1. Bert, 2. Roberta, 3. Bart,\n",
        "# Create a directory to save the models\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "def apply_sampling(X_train,y_train, which_sample):\n",
        "  if which_sample == 0:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "    return X_train_resampled, y_train_resampled\n",
        "  elif which_sample == 1:\n",
        "    tomek = TomekLinks()\n",
        "    X_train_resampled_tomek, y_train_resampled_tomek = tomek.fit_resample(X_train, y_train)\n",
        "    return X_train_resampled_tomek, y_train_resampled_tomek\n",
        "  elif which_sample == 2:\n",
        "    ncl = NeighbourhoodCleaningRule()\n",
        "    X_train_resampled_tomek_ncl, y_train_resampled_tomek_ncl = ncl.fit_resample(X_train,\n",
        "                                                                                y_train)\n",
        "    return X_train_resampled_tomek_ncl, y_train_resampled_tomek_ncl\n",
        "\n",
        "  else:\n",
        "    print(\"specify the sampling technique\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset and preprocess it\n",
        "def load_and_preprocess_data():\n",
        "    # Load the IMDB movie reviews dataset\n",
        "    #imdb_dataset = load_dataset(\"imdb\")\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Twitch_Colab/twitch_messages.csv')\n",
        "    # df2 = pd.read_csv('/content/drive/MyDrive/Twitch_Colab/data.csv')\n",
        "    #1. replace @name with @author, if any\n",
        "    unique_values = df['StreamerName'].unique().tolist()\n",
        "\n",
        "    df['replaced_author'] = df['message']\n",
        "    #display how many?\n",
        "\n",
        "    unique_values = df['StreamerName'].unique().tolist()\n",
        "    for item in unique_values:\n",
        "        df['replaced_author'] = df['replaced_author'].str.replace(item, 'author')\n",
        "    unique_values = [x.lower() for x in unique_values]\n",
        "    for item in unique_values:\n",
        "        df['replaced_author'] = df['replaced_author'].str.replace(item.lower(), 'author')\n",
        "\n",
        "\n",
        "    #print(\"author name was replaced with @author {0} times\".format(df['replaced_author'].str.count('author').sum()))\n",
        "    # 2. Remove all spaces on message itself, without replacing author name\n",
        "    df['no_spaces'] = df['message'].str.replace(r'\\s+', ' ')\n",
        "    # 3. Remove punctuation, maybe we should not remove it? since some are about questions?\n",
        "    df['no_punctuation'] =  df['no_spaces'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "\n",
        "    #df['stop_words'] = df['messages'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stopwords.words('english')]))\n",
        "\n",
        "    new_df = pd.DataFrame({\n",
        "        'video_id': df['VidID'],\n",
        "        'streamer': df['StreamerName'],\n",
        "        'message': df['message'],\n",
        "        'replaced_author': df['replaced_author'],\n",
        "        'no_spaces': df['no_spaces'],\n",
        "        'no_punctuation':  df['no_punctuation'],\n",
        "        'label': df['Mohammad_Code']\n",
        "    })\n",
        "\n",
        "    # 4. Remove duplicate messages\n",
        "    new_df = new_df.drop_duplicates(subset=['no_spaces'],keep='first')\n",
        "\n",
        "    # 5. make it balanced, simple \n",
        "    new_df = new_df.dropna()\n",
        "    new_df['replaced_author'] = new_df['replaced_author'].apply(lambda x: x.replace(\"@\", \"@user\") if isinstance(x, str) and \"@\" in x and \"@author\" not in x else x)\n",
        "\n",
        "    pattern = r\"@\\w+\"\n",
        "    for i, row in new_df.iterrows():\n",
        "        message = row['replaced_author']\n",
        "        if isinstance(message, str) and \"@user\" in message:\n",
        "            # replace any occurrence of \"@userstring\" with \"@user\"\n",
        "            message = re.sub(pattern, \"@user\", message)\n",
        "            #print(message)\n",
        "            # strip \"user\" from \"@user\"\n",
        "            #message = message.replace(\"@user\", \"@\")\n",
        "            # update the message in the DataFrame\n",
        "            df.loc[i, 'replaced_author'] = message\n",
        "\n",
        "    #label_counts = new_df['label'].value_counts()\n",
        "    print(\"before sampling\")\n",
        "    count_rel = (new_df['label'] == 0).sum()\n",
        "    count_irr = (new_df['label'] == 1).sum()\n",
        "    print(\"we have {0} relevant to streamer messages, and {1} irrelevant to streamer messages\".format(count_rel, count_irr))\n",
        "    # Sampling\n",
        "    # g = new_df.groupby('label')\n",
        "    # new_df = g.apply(lambda x: x.sample(g.size().min(), random_state=1))\n",
        "\n",
        "\n",
        "    print(\"author name was replaced with @author {0} times\".format(df['replaced_author'].str.count('@author').sum()))\n",
        "    print(\"user name was replaced with @user {0} times\".format(df['replaced_author'].str.count('@user').sum()))\n",
        "\n",
        "    label_counts = new_df['label'].value_counts()\n",
        "    #print(\"sampling\", label_counts)\n",
        "    \n",
        "    # Combine the 'train' and 'test' splits into a single dataset\n",
        "    # We can change the the pre-processing here\n",
        "    dataset = {\n",
        "        'text': new_df['no_spaces'].tolist(),\n",
        "        'labels': new_df['label'].tolist()\n",
        "    }\n",
        "\n",
        "    # print(len(dataset[\"labels\"]))\n",
        "    # dataset = {\n",
        "    #   'text': df2['text'].tolist(),\n",
        "    #   'labels': df2['label'].tolist()\n",
        "    #   }\n",
        "    print(\"After Sampling\")\n",
        "    count_rel = (new_df['label'] == 0).sum()\n",
        "    count_irr = (new_df['label'] == 1).sum()\n",
        "    print(\"we have {0} relevant to streamer messages, and {1} irrelevant to streamer messages\".format(count_rel, count_irr))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Custom dataset class\n",
        "class BinaryClassificationDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Function to encode the dataset\n",
        "def encode_dataset(tokenizer, texts, labels, max_length=512):\n",
        "    encoded_data = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
        "    return BinaryClassificationDataset(encoded_data, labels)\n",
        "\n",
        "\n",
        "def predict(text, model, tokenizer):\n",
        "    # Preprocess the input text\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "    # Move the model to the same device as the input tensors\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the predicted class (0 or 1)\n",
        "    predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# load_and_preprocess_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 646,
          "referenced_widgets": [
            "4524a942c3234304a19aaf75e091844e",
            "59e6df70aeca4eefb7b9d1086cb42257",
            "e95e8ac29eae4def83f9cb870b1ea0b6",
            "6b5661979c5c4b46834af2527024bafe",
            "148df3cf9b4e44058816a6c3bad96574",
            "5ff5602756e447bf8480e2aa94afa523",
            "b31aed77fef24184a8ec7528a78fb417",
            "1498864b590a4d3b980fa1ae2571d17b",
            "0bc578f5be0b4cd59439584d80ae0228",
            "da817f6ce6df49119742dbd98e675871",
            "41346455c06e4af38b2cc2d9a93a1fb5",
            "8869be71656e4b5aa237261575b5f4dc",
            "d35bf42f73794ef6a6be7518986a4efc",
            "142fe79ea78045ffb9db9e7c8cecac93",
            "60cb44290ccb48128a80f07d8c87e1d3",
            "0da911fe49544900ac0e9686ccb7e2b9",
            "10d470bf68c24edbbc010613f817fc3f",
            "9e13b4083b324af69f68d2b003be0c0d",
            "4a5258f3c56543bdb479f9ed811cc71e"
          ]
        },
        "id": "M60tZyosMhlp",
        "outputId": "8d127879-ee7f-4f3f-9172-571ee98d73d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unprocessed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4524a942c3234304a19aaf75e091844e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59e6df70aeca4eefb7b9d1086cb42257",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)ve/main/spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e95e8ac29eae4def83f9cb870b1ea0b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-6e3c49d82591>:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_spaces'] = df['replaced_author'].str.replace(r'\\s+', ' ')\n",
            "<ipython-input-5-6e3c49d82591>:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_punctuation'] =  df['no_spaces'].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "author name was replaced with @author 36.0 times\n",
            "user name was replaced with @user 99.0 times\n",
            "After Sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fcb37580>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b5661979c5c4b46834af2527024bafe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.516000</td>\n",
              "      <td>0.524714</td>\n",
              "      <td>0.743025</td>\n",
              "      <td>0.823699</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.765101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.504700</td>\n",
              "      <td>0.577182</td>\n",
              "      <td>0.709251</td>\n",
              "      <td>0.873606</td>\n",
              "      <td>0.588972</td>\n",
              "      <td>0.703593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.501800</td>\n",
              "      <td>0.470186</td>\n",
              "      <td>0.791483</td>\n",
              "      <td>0.800937</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.828087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc08e460>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 13:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.624200</td>\n",
              "      <td>0.642852</td>\n",
              "      <td>0.688693</td>\n",
              "      <td>0.665480</td>\n",
              "      <td>0.939698</td>\n",
              "      <td>0.779167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.489700</td>\n",
              "      <td>0.545890</td>\n",
              "      <td>0.770925</td>\n",
              "      <td>0.821809</td>\n",
              "      <td>0.776382</td>\n",
              "      <td>0.798450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.402800</td>\n",
              "      <td>0.478984</td>\n",
              "      <td>0.785609</td>\n",
              "      <td>0.811881</td>\n",
              "      <td>0.824121</td>\n",
              "      <td>0.817955</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc893b80>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 13:05, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.645400</td>\n",
              "      <td>0.546823</td>\n",
              "      <td>0.757709</td>\n",
              "      <td>0.831909</td>\n",
              "      <td>0.733668</td>\n",
              "      <td>0.779706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.520800</td>\n",
              "      <td>0.648408</td>\n",
              "      <td>0.728341</td>\n",
              "      <td>0.766917</td>\n",
              "      <td>0.768844</td>\n",
              "      <td>0.767880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.542100</td>\n",
              "      <td>0.495241</td>\n",
              "      <td>0.762115</td>\n",
              "      <td>0.801020</td>\n",
              "      <td>0.788945</td>\n",
              "      <td>0.794937</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e7f4a7c0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 13:05, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.475600</td>\n",
              "      <td>0.567657</td>\n",
              "      <td>0.737151</td>\n",
              "      <td>0.724846</td>\n",
              "      <td>0.886935</td>\n",
              "      <td>0.797740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.533000</td>\n",
              "      <td>0.597688</td>\n",
              "      <td>0.627019</td>\n",
              "      <td>0.818584</td>\n",
              "      <td>0.464824</td>\n",
              "      <td>0.592949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.518000</td>\n",
              "      <td>0.588612</td>\n",
              "      <td>0.719530</td>\n",
              "      <td>0.718816</td>\n",
              "      <td>0.854271</td>\n",
              "      <td>0.780712</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "680\n",
            "2724\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc08e520>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']\n",
            "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 13:05, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.477000</td>\n",
              "      <td>0.555090</td>\n",
              "      <td>0.702941</td>\n",
              "      <td>0.673145</td>\n",
              "      <td>0.957286</td>\n",
              "      <td>0.790456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.602500</td>\n",
              "      <td>0.577799</td>\n",
              "      <td>0.717647</td>\n",
              "      <td>0.701961</td>\n",
              "      <td>0.899497</td>\n",
              "      <td>0.788546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.472500</td>\n",
              "      <td>0.501539</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.830189</td>\n",
              "      <td>0.773869</td>\n",
              "      <td>0.801040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampling technique:  0\n",
            "Model's Results of  albert-base-v2\n",
            "Accuracy: 0.766747430249633 (¬±0.025638535237042438)\n",
            "Precision: 0.792568622266104 (¬±0.03839047212325879)\n",
            "Recall: 0.8196697774587222 (¬±0.03365418021302768)\n",
            "F1-score: 0.8045462251464002 (¬±0.016773115607155175)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "148df3cf9b4e44058816a6c3bad96574",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ff5602756e447bf8480e2aa94afa523",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b31aed77fef24184a8ec7528a78fb417",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1498864b590a4d3b980fa1ae2571d17b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-6e3c49d82591>:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_spaces'] = df['replaced_author'].str.replace(r'\\s+', ' ')\n",
            "<ipython-input-5-6e3c49d82591>:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_punctuation'] =  df['no_spaces'].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "author name was replaced with @author 36.0 times\n",
            "user name was replaced with @user 99.0 times\n",
            "After Sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9dd950160>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bc578f5be0b4cd59439584d80ae0228",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.505700</td>\n",
              "      <td>0.508064</td>\n",
              "      <td>0.760646</td>\n",
              "      <td>0.744813</td>\n",
              "      <td>0.899749</td>\n",
              "      <td>0.814983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.349000</td>\n",
              "      <td>0.482880</td>\n",
              "      <td>0.785609</td>\n",
              "      <td>0.862464</td>\n",
              "      <td>0.754386</td>\n",
              "      <td>0.804813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.195400</td>\n",
              "      <td>0.834796</td>\n",
              "      <td>0.801762</td>\n",
              "      <td>0.856757</td>\n",
              "      <td>0.794486</td>\n",
              "      <td>0.824447</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc08e520>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.444900</td>\n",
              "      <td>0.501726</td>\n",
              "      <td>0.790015</td>\n",
              "      <td>0.786517</td>\n",
              "      <td>0.879397</td>\n",
              "      <td>0.830368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.447700</td>\n",
              "      <td>0.496214</td>\n",
              "      <td>0.788546</td>\n",
              "      <td>0.901899</td>\n",
              "      <td>0.716080</td>\n",
              "      <td>0.798319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.271400</td>\n",
              "      <td>0.715610</td>\n",
              "      <td>0.801762</td>\n",
              "      <td>0.831234</td>\n",
              "      <td>0.829146</td>\n",
              "      <td>0.830189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e7dba9d0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.514000</td>\n",
              "      <td>0.431351</td>\n",
              "      <td>0.809104</td>\n",
              "      <td>0.866120</td>\n",
              "      <td>0.796482</td>\n",
              "      <td>0.829843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.312000</td>\n",
              "      <td>0.467622</td>\n",
              "      <td>0.810573</td>\n",
              "      <td>0.889855</td>\n",
              "      <td>0.771357</td>\n",
              "      <td>0.826380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.259100</td>\n",
              "      <td>0.822159</td>\n",
              "      <td>0.803231</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.814070</td>\n",
              "      <td>0.828645</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e63d27c0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.389600</td>\n",
              "      <td>0.506255</td>\n",
              "      <td>0.757709</td>\n",
              "      <td>0.845697</td>\n",
              "      <td>0.716080</td>\n",
              "      <td>0.775510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.411000</td>\n",
              "      <td>0.562228</td>\n",
              "      <td>0.751836</td>\n",
              "      <td>0.856698</td>\n",
              "      <td>0.690955</td>\n",
              "      <td>0.764951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.326400</td>\n",
              "      <td>0.851908</td>\n",
              "      <td>0.797357</td>\n",
              "      <td>0.829949</td>\n",
              "      <td>0.821608</td>\n",
              "      <td>0.825758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "680\n",
            "2724\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e7e72460>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.445600</td>\n",
              "      <td>0.476601</td>\n",
              "      <td>0.766176</td>\n",
              "      <td>0.747412</td>\n",
              "      <td>0.907035</td>\n",
              "      <td>0.819523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.305100</td>\n",
              "      <td>0.477380</td>\n",
              "      <td>0.802941</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.874372</td>\n",
              "      <td>0.838554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.201800</td>\n",
              "      <td>0.732894</td>\n",
              "      <td>0.802941</td>\n",
              "      <td>0.856757</td>\n",
              "      <td>0.796482</td>\n",
              "      <td>0.825521</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampling technique:  0\n",
            "Model's Results of  bert-base-uncased\n",
            "Accuracy: 0.8014105554115918 (¬±0.00211353016548245)\n",
            "Precision: 0.8436894018038291 (¬±0.011708440736756656)\n",
            "Recall: 0.8111585496404328 (¬±0.013671611402435255)\n",
            "F1-score: 0.8269117847630449 (¬±0.00214852530150938)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da817f6ce6df49119742dbd98e675871",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41346455c06e4af38b2cc2d9a93a1fb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8869be71656e4b5aa237261575b5f4dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d35bf42f73794ef6a6be7518986a4efc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-6e3c49d82591>:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_spaces'] = df['replaced_author'].str.replace(r'\\s+', ' ')\n",
            "<ipython-input-5-6e3c49d82591>:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_punctuation'] =  df['no_spaces'].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "author name was replaced with @author 36.0 times\n",
            "user name was replaced with @user 99.0 times\n",
            "After Sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc76a910>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "142fe79ea78045ffb9db9e7c8cecac93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.569993</td>\n",
              "      <td>0.779736</td>\n",
              "      <td>0.826772</td>\n",
              "      <td>0.789474</td>\n",
              "      <td>0.807692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.551200</td>\n",
              "      <td>0.530104</td>\n",
              "      <td>0.781204</td>\n",
              "      <td>0.867647</td>\n",
              "      <td>0.739348</td>\n",
              "      <td>0.798376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.303100</td>\n",
              "      <td>0.781603</td>\n",
              "      <td>0.787078</td>\n",
              "      <td>0.850829</td>\n",
              "      <td>0.771930</td>\n",
              "      <td>0.809461</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9dda3ae20>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.485700</td>\n",
              "      <td>0.485661</td>\n",
              "      <td>0.779736</td>\n",
              "      <td>0.784404</td>\n",
              "      <td>0.859296</td>\n",
              "      <td>0.820144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.501600</td>\n",
              "      <td>0.589884</td>\n",
              "      <td>0.773862</td>\n",
              "      <td>0.912162</td>\n",
              "      <td>0.678392</td>\n",
              "      <td>0.778098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.526300</td>\n",
              "      <td>0.755878</td>\n",
              "      <td>0.784141</td>\n",
              "      <td>0.832891</td>\n",
              "      <td>0.788945</td>\n",
              "      <td>0.810323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc91a3d0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.424000</td>\n",
              "      <td>0.446311</td>\n",
              "      <td>0.797357</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.816583</td>\n",
              "      <td>0.824873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.226900</td>\n",
              "      <td>0.638070</td>\n",
              "      <td>0.795888</td>\n",
              "      <td>0.903427</td>\n",
              "      <td>0.728643</td>\n",
              "      <td>0.806676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.384400</td>\n",
              "      <td>0.666440</td>\n",
              "      <td>0.807636</td>\n",
              "      <td>0.878187</td>\n",
              "      <td>0.778894</td>\n",
              "      <td>0.825566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e6254be0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.448200</td>\n",
              "      <td>0.595304</td>\n",
              "      <td>0.765051</td>\n",
              "      <td>0.838068</td>\n",
              "      <td>0.741206</td>\n",
              "      <td>0.786667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.380300</td>\n",
              "      <td>0.641570</td>\n",
              "      <td>0.747430</td>\n",
              "      <td>0.889655</td>\n",
              "      <td>0.648241</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.233800</td>\n",
              "      <td>0.781458</td>\n",
              "      <td>0.792952</td>\n",
              "      <td>0.848238</td>\n",
              "      <td>0.786432</td>\n",
              "      <td>0.816167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "680\n",
            "2724\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e63325b0>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 12:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.455000</td>\n",
              "      <td>0.520560</td>\n",
              "      <td>0.723529</td>\n",
              "      <td>0.691606</td>\n",
              "      <td>0.952261</td>\n",
              "      <td>0.801268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.543700</td>\n",
              "      <td>0.576783</td>\n",
              "      <td>0.773529</td>\n",
              "      <td>0.797561</td>\n",
              "      <td>0.821608</td>\n",
              "      <td>0.809406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.271500</td>\n",
              "      <td>0.527532</td>\n",
              "      <td>0.819118</td>\n",
              "      <td>0.849873</td>\n",
              "      <td>0.839196</td>\n",
              "      <td>0.844501</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:18]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampling technique:  0\n",
            "Model's Results of  roberta-base\n",
            "Accuracy: 0.798184762891941 (¬±0.01323243800984067)\n",
            "Precision: 0.8520036401452729 (¬±0.014635215791132358)\n",
            "Recall: 0.793079432248964 (¬±0.023818215687438984)\n",
            "F1-score: 0.8212034488107808 (¬±0.012987915101509623)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60cb44290ccb48128a80f07d8c87e1d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0da911fe49544900ac0e9686ccb7e2b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10d470bf68c24edbbc010613f817fc3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e13b4083b324af69f68d2b003be0c0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-6e3c49d82591>:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_spaces'] = df['replaced_author'].str.replace(r'\\s+', ' ')\n",
            "<ipython-input-5-6e3c49d82591>:54: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['no_punctuation'] =  df['no_spaces'].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "before sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "author name was replaced with @author 36.0 times\n",
            "user name was replaced with @user 99.0 times\n",
            "After Sampling\n",
            "we have 1413 relevant to streamer messages, and 1991 irrelevant to streamer messages\n",
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e7dd4940>\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a5258f3c56543bdb479f9ed811cc71e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 06:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.502200</td>\n",
              "      <td>0.472093</td>\n",
              "      <td>0.784141</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.917293</td>\n",
              "      <td>0.832765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.384900</td>\n",
              "      <td>0.442990</td>\n",
              "      <td>0.797357</td>\n",
              "      <td>0.857534</td>\n",
              "      <td>0.784461</td>\n",
              "      <td>0.819372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.139800</td>\n",
              "      <td>0.725687</td>\n",
              "      <td>0.810573</td>\n",
              "      <td>0.860963</td>\n",
              "      <td>0.807018</td>\n",
              "      <td>0.833118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e63d2970>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 06:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.485600</td>\n",
              "      <td>0.450071</td>\n",
              "      <td>0.772394</td>\n",
              "      <td>0.754717</td>\n",
              "      <td>0.904523</td>\n",
              "      <td>0.822857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.359800</td>\n",
              "      <td>0.519205</td>\n",
              "      <td>0.779736</td>\n",
              "      <td>0.889937</td>\n",
              "      <td>0.711055</td>\n",
              "      <td>0.790503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.290700</td>\n",
              "      <td>0.818716</td>\n",
              "      <td>0.787078</td>\n",
              "      <td>0.828571</td>\n",
              "      <td>0.801508</td>\n",
              "      <td>0.814815</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc07c250>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 06:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.450200</td>\n",
              "      <td>0.442777</td>\n",
              "      <td>0.804699</td>\n",
              "      <td>0.838875</td>\n",
              "      <td>0.824121</td>\n",
              "      <td>0.831432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.227300</td>\n",
              "      <td>0.484026</td>\n",
              "      <td>0.814978</td>\n",
              "      <td>0.867568</td>\n",
              "      <td>0.806533</td>\n",
              "      <td>0.835938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.208300</td>\n",
              "      <td>0.815695</td>\n",
              "      <td>0.809104</td>\n",
              "      <td>0.848958</td>\n",
              "      <td>0.819095</td>\n",
              "      <td>0.833760</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "681\n",
            "2723\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9e7db0430>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 06:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.397000</td>\n",
              "      <td>0.538385</td>\n",
              "      <td>0.745962</td>\n",
              "      <td>0.818697</td>\n",
              "      <td>0.726131</td>\n",
              "      <td>0.769640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.455800</td>\n",
              "      <td>0.541642</td>\n",
              "      <td>0.765051</td>\n",
              "      <td>0.841954</td>\n",
              "      <td>0.736181</td>\n",
              "      <td>0.785523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.132800</td>\n",
              "      <td>0.907143</td>\n",
              "      <td>0.779736</td>\n",
              "      <td>0.829787</td>\n",
              "      <td>0.783920</td>\n",
              "      <td>0.806202</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [86/86 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "680\n",
            "2724\n",
            "<__main__.BinaryClassificationDataset object at 0x7fe9fc83ff10>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 06:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.410800</td>\n",
              "      <td>0.466060</td>\n",
              "      <td>0.755882</td>\n",
              "      <td>0.740664</td>\n",
              "      <td>0.896985</td>\n",
              "      <td>0.811364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.329400</td>\n",
              "      <td>0.453499</td>\n",
              "      <td>0.802941</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.829146</td>\n",
              "      <td>0.831234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.092900</td>\n",
              "      <td>0.708152</td>\n",
              "      <td>0.816176</td>\n",
              "      <td>0.864000</td>\n",
              "      <td>0.814070</td>\n",
              "      <td>0.838292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [85/85 00:10]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampling technique:  0\n",
            "Model's Results of  distilbert-base-uncased\n",
            "Accuracy: 0.8005333851602314 (¬±0.014370456164907897)\n",
            "Precision: 0.846455912558447 (¬±0.01498131999142143)\n",
            "Recall: 0.8051221017367538 (¬±0.012178097920051747)\n",
            "F1-score: 0.825237209310305 (¬±0.012459159321526487)\n"
          ]
        }
      ],
      "source": [
        "# Define which model will we be using\n",
        "#done the first one\n",
        "\n",
        "models = [\"setfit-distilbert-user-intent\"] \n",
        "models = [\"albert-base-v2\", \"bert-base-uncased\" , \"roberta-base\", \"distilbert-base-uncased\"]\n",
        "# models = [\"bert-base-uncased\"]\n",
        "sampling_technique = 0\n",
        "print(\"Unprocessed\")\n",
        "for m in models:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "\n",
        "  # Load and preprocess the dataset\n",
        "  dataset = load_and_preprocess_data()\n",
        "\n",
        "  # Initialize StratifiedKFold with 10 folds\n",
        "  n_splits = 5\n",
        "  kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "  # Collect the evaluation metrics across all folds\n",
        "  accuracy_list = []\n",
        "  precision_list = []\n",
        "  recall_list = []\n",
        "  f1_list = []\n",
        "  smote = SMOTE(random_state=42)\n",
        "\n",
        "  for train_index, val_index in kfold.split(dataset['text'], dataset['labels']):\n",
        "      train_texts = [dataset['text'][i] for i in train_index]\n",
        "      train_labels = [dataset['labels'][i] for i in train_index]\n",
        "      # train_texts, train_labels = apply_sampling(train_texts, train_labels, sampling_technique)\n",
        "      print(len(val_index))\n",
        "      print(len(train_index))\n",
        "\n",
        "\n",
        "      val_texts = [dataset['text'][i] for i in val_index]\n",
        "      val_labels = [dataset['labels'][i] for i in val_index]\n",
        "     \n",
        "      # Encode the datasets\n",
        "      train_encodings = encode_dataset(tokenizer, train_texts, train_labels)\n",
        "      val_encodings = encode_dataset(tokenizer, val_texts, val_labels)\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      print(train_encodings)\n",
        "      #encodings = encode_dataset(tokenizer, dataset['text'], dataset['labels'])\n",
        "      #train_encodings, train_labels = smote.fit_resample(train_encodings, train_labels)\n",
        "      #X_resampled, y_resampled = smote.fit_resample(encodings['text'], encodings['labels'])\n",
        "\n",
        "      # Prepare the training arguments\n",
        "      training_args = TrainingArguments(\n",
        "          output_dir='./results',\n",
        "          num_train_epochs=3,\n",
        "          per_device_train_batch_size=8,\n",
        "          per_device_eval_batch_size=8,\n",
        "          warmup_steps=500,\n",
        "          weight_decay=0.01,\n",
        "          logging_dir='./logs',\n",
        "          logging_steps=10,\n",
        "          evaluation_strategy=\"epoch\",\n",
        "      )\n",
        "\n",
        "      # Define the model\n",
        "      model = AutoModelForSequenceClassification.from_pretrained(m, num_labels=2)\n",
        "\n",
        "      # Create the Trainer\n",
        "      trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          train_dataset=train_encodings,\n",
        "          eval_dataset=val_encodings,\n",
        "          compute_metrics=lambda eval_pred: {\n",
        "            'accuracy': accuracy_score(eval_pred.label_ids, eval_pred.predictions.argmax(-1)),\n",
        "            'precision': precision_score(eval_pred.label_ids, eval_pred.predictions.argmax(-1)),\n",
        "            'recall': recall_score(eval_pred.label_ids, eval_pred.predictions.argmax(-1)),\n",
        "            'f1': f1_score(eval_pred.label_ids, eval_pred.predictions.argmax(-1)),\n",
        "          }\n",
        "      )\n",
        "\n",
        "      # Train and evaluate the model\n",
        "      trainer.train()\n",
        "      eval_metrics = trainer.evaluate()\n",
        "\n",
        "      # Store the evaluation metrics\n",
        "      accuracy_list.append(eval_metrics['eval_accuracy'])\n",
        "      precision_list.append(eval_metrics['eval_precision'])\n",
        "      recall_list.append(eval_metrics['eval_recall'])\n",
        "      f1_list.append(eval_metrics['eval_f1'])\n",
        "\n",
        "      # Save the trained model\n",
        "      # model_save_path = f\"saved_models/model_fold_{fold + 1}\"\n",
        "      # model.save_pretrained(model_save_path)\n",
        "\n",
        "  # Print the standard evaluation metrics\n",
        "  print(\"sampling technique: \", sampling_technique)\n",
        "  print(\"Model's Results of \", m)\n",
        "  print(f\"Accuracy: {np.mean(accuracy_list)} (¬±{np.std(accuracy_list)})\")\n",
        "  print(f\"Precision: {np.mean(precision_list)} (¬±{np.std(precision_list)})\")\n",
        "  print(f\"Recall: {np.mean(recall_list)} (¬±{np.std(recall_list)})\")\n",
        "  print(f\"F1-score: {np.mean(f1_list)} (¬±{np.std(f1_list)})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lRE1ROAsdBw"
      },
      "outputs": [],
      "source": [
        "#poche replication\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load data from CSV file\n",
        "data = load_and_preprocess_data()\n",
        "X = data['message']\n",
        "y = data['label']\n",
        "\n",
        "# Define the stemmer and stopwords for preprocessing\n",
        "stemmer = SnowballStemmer('english')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define the vectorizers for the three preprocessing methods\n",
        "vectorizers = [\n",
        "    ('No Preprocessing', CountVectorizer(), None),\n",
        "    ('Stemming', CountVectorizer(analyzer=stemmer.stem), stemmer),\n",
        "    ('Stopwords Removal and Stemming', CountVectorizer(analyzer=stemmer.stem, stop_words=stop_words), stemmer)\n",
        "]\n",
        "\n",
        "# Define the classifiers\n",
        "classifiers = [\n",
        "    ('Naive Bayes', MultinomialNB()),\n",
        "    ('SVM', LinearSVC())\n",
        "]\n",
        "\n",
        "# Define the evaluation metrics\n",
        "metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "\n",
        "# Perform 10-fold cross-validation for each preprocessing method and each classifier\n",
        "for vectorizer_name, vectorizer, stemmer in vectorizers:\n",
        "    X_processed = X.apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]) if stemmer else x)\n",
        "    X_processed = vectorizer.fit_transform(X_processed)\n",
        "\n",
        "    for classifier_name, classifier in classifiers:\n",
        "        pipeline = make_pipeline(vectorizer, classifier)\n",
        "        scores = cross_val_score(pipeline, X, y, cv=KFold(n_splits=10, shuffle=True, random_state=42), scoring=metrics)\n",
        "        print(f\"{vectorizer_name} with {classifier_name}:\\n\"\n",
        "              f\"Accuracy: {scores.mean(axis=0)[0]:.4f}\\n\"\n",
        "              f\"Precision: {scores.mean(axis=0)[1]:.4f}\\n\"\n",
        "              f\"Recall: {scores.mean(axis=0)[2]:.4f}\\n\"\n",
        "              f\"F1-score: {scores.mean(axis=0)[3]:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WqKv9S6QK29"
      },
      "outputs": [],
      "source": [
        "dataset = load_and_preprocess_data()\n",
        "\n",
        "# initialize the BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# define the number of cross-validation folds\n",
        "num_folds = 5\n",
        "\n",
        "# define the sampling techniques\n",
        "sampling_techniques = [\n",
        "    ('SMOTE', SMOTE()),\n",
        "    ('TomekLinks', TomekLinks()),\n",
        "    ('NeighbourhoodCleaningRule', NeighbourhoodCleaningRule())\n",
        "]\n",
        "\n",
        "# define the evaluation metrics\n",
        "metrics = ['accuracy', 'precision', 'recall', 'f1-score']\n",
        "\n",
        "# define the k-fold cross-validation strategy\n",
        "skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# initialize the arrays to store the evaluation results for each fold and each sampling technique\n",
        "results = {technique[0]: {metric: [] for metric in metrics} for technique in sampling_techniques}\n",
        "\n",
        "# perform k-fold cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(skf.split(dataset['text'], dataset['labels'])):\n",
        "    # extract the training and validation sets for the current fold\n",
        "\n",
        "   \n",
        "    X_train = [dataset['text'][i] for i in train_index]\n",
        "    y_train = [dataset['labels'][i] for i in train_index]\n",
        "    \n",
        "    X_train = np.array(X_train)\n",
        "    X_train = np.reshape(X_train, (-1, 1))\n",
        "    print(X_train.shape)\n",
        "    y_train = np.array(y_train)\n",
        "    y_train = np.reshape(y_train, (-1, 1))\n",
        "    print(y_train)\n",
        "\n",
        "    X_val, y_val = [dataset['text'][i] for i in val_index], [dataset['labels'][i] for i in val_index]\n",
        "\n",
        "    # apply each sampling technique to the training set\n",
        "    for technique_name, sampler in sampling_techniques:\n",
        "        # x_train = np.array(X_train).reshape(-1, 1)\n",
        "        # y_train =  np.array(y_train).reshape(-1, 1)\n",
        "        #X_val = np.array(X_val).reshape(-1, 1)\n",
        "        X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        # tokenize the resampled training and validation sets\n",
        "        train_inputs = tokenizer(X_train_resampled.tolist(), padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
        "        val_inputs = tokenizer(X_val.tolist(), padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
        "\n",
        "        # train the BERT model on the resampled training set and evaluate it on the validation set\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=model.compute_loss, metrics=['accuracy'])\n",
        "        model.fit(train_inputs, y_train_resampled, validation_data=(val_inputs, y_val), batch_size=16, epochs=3)\n",
        "\n",
        "        # predict the labels for the validation set and compute the evaluation metrics\n",
        "        y_pred = model.predict(val_inputs, batch_size=16)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred_classes, average='binary')\n",
        "\n",
        "        # store the evaluation results for the current fold and sampling technique\n",
        "        results[technique_name]['accuracy'].append(accuracy)\n",
        "        results[technique_name]['precision'].append(precision)\n",
        "        results[technique_name]['recall'].append(recall)\n",
        "        results[technique_name]['f1-score'].append(f1)\n",
        "\n",
        "# compute the mean evaluation metrics for each sampling technique\n",
        "for technique_name, _ in sampling_techniques:\n",
        "    print(f\"Mean evaluation metrics for {technique_name}:\")\n",
        "    for metric in metrics:\n",
        "        mean_metric = np.mean(results[technique_name][metric])\n",
        "        print(f\"{metric}: {mean_metric:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iws2Nze2NoHb"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkKQCOXfNoRq"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}